# クイックスタートチュートリアル: 特定領域の星のNEOWISEデータ取得からビューワー表示まで

## 概要

このチュートリアルでは、以下の一連の作業を説明します：

1. **ターゲット座標周辺の星を抽出** - DataFrameから25分角以内の星を抽出
2. **NEOWISEデータを取得** - 抽出した星のNEOWISEデータを取得
3. **SQLiteに保存** - データをSQLiteデータベースに保存
4. **ビューワーで表示** - Webツールでライトカーブを表示

### ターゲット座標
- **RA**: 18h09m01.4800s = 272.256度
- **Dec**: -20d05m08.000s = -20.0856度
- **検索半径**: 25分角

---

## 必要な環境

```python
# 必要なパッケージ
pip install pandas numpy astropy astroquery tqdm
```

---

## Step 1: ターゲット座標周辺の星を抽出

### セル1: インポートと座標変換

```python
import pandas as pd
import numpy as np
from astropy import units as u
from astropy.coordinates import SkyCoord
import sqlite3
from astroquery.ipac.irsa import Irsa
from tqdm.notebook import tqdm

# ターゲット座標（18h09m01.4800s, -20d05m08.000s）
target_ra_str = "18h09m01.4800s"
target_dec_str = "-20d05m08.000s"

# SkyCoordで座標変換
target_coord = SkyCoord(target_ra_str, target_dec_str, frame='icrs')
target_ra = target_coord.ra.degree   # 272.256度
target_dec = target_coord.dec.degree  # -20.0856度

print(f"ターゲット座標:")
print(f"  RA:  {target_ra_str} = {target_ra:.6f}度")
print(f"  Dec: {target_dec_str} = {target_dec:.6f}度")
```

### セル2: DataFrameから25分角以内の星を抽出

```python
# 検索半径（分角 → 度）
search_radius_arcmin = 25
search_radius_deg = search_radius_arcmin / 60.0

print(f"検索半径: {search_radius_arcmin}分角 = {search_radius_deg:.4f}度")

def extract_stars_within_radius(df, target_ra, target_dec, radius_deg):
    """
    DataFrameからターゲット座標周辺の星を抽出
    
    Parameters:
    -----------
    df : pd.DataFrame
        source_id, ra, dec カラムを持つDataFrame
    target_ra : float
        ターゲットの赤経（度）
    target_dec : float
        ターゲットの赤緯（度）
    radius_deg : float
        検索半径（度）
    
    Returns:
    --------
    pd.DataFrame
        抽出された星のDataFrame
    """
    # 球面上の角距離計算（簡易版、小角度近似）
    # より正確な計算にはhaversine公式を使用
    ra_diff = (df['ra'] - target_ra) * np.cos(np.radians(target_dec))
    dec_diff = df['dec'] - target_dec
    angular_sep = np.sqrt(ra_diff**2 + dec_diff**2)
    
    # 半径以内の星を抽出
    mask = angular_sep <= radius_deg
    extracted = df[mask].copy()
    
    # 角距離を追加
    extracted['angular_sep_arcmin'] = angular_sep[mask] * 60
    
    return extracted.sort_values('angular_sep_arcmin')


# --- 例: CSVファイルから星カタログを読み込む場合 ---
# your_dataframe = pd.read_csv('your_star_catalog.csv')

# --- 例: 仮のDataFrameを作成（実際のデータに置き換えてください） ---
# 以下は例です。実際のDataFrameに置き換えてください。
your_dataframe = pd.DataFrame({
    'source_id': [
        '4098765432109876543', '4098765432109876544', '4098765432109876545',
        '4098765432109876546', '4098765432109876547', '4098765432109876548'
    ],
    'ra': [272.26, 272.30, 272.20, 272.50, 272.10, 273.00],
    'dec': [-20.08, -20.10, -20.05, -20.20, -19.90, -20.50],
    'AllWISE_ID': [
        'J180901.44-200504.8', 'J180912.00-200600.0', 'J180848.00-200300.0',
        'J181000.00-201200.0', 'J180824.00-195400.0', 'J181200.00-203000.0'
    ]
})

# 25分角以内の星を抽出
extracted_stars = extract_stars_within_radius(
    your_dataframe, 
    target_ra, 
    target_dec, 
    search_radius_deg
)

print(f"\n抽出された星の数: {len(extracted_stars)}")
print(f"\n抽出された星:")
print(extracted_stars)
```

### セル3: より正確な球面距離計算（オプション）

```python
def extract_stars_haversine(df, target_ra, target_dec, radius_deg):
    """
    Haversine公式を使った正確な球面距離計算
    """
    # 度をラジアンに変換
    ra1, dec1 = np.radians(target_ra), np.radians(target_dec)
    ra2, dec2 = np.radians(df['ra'].values), np.radians(df['dec'].values)
    
    # Haversine公式
    dra = ra2 - ra1
    ddec = dec2 - dec1
    a = np.sin(ddec/2)**2 + np.cos(dec1) * np.cos(dec2) * np.sin(dra/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    
    # 角距離（度）
    angular_sep_deg = np.degrees(c)
    
    # 半径以内の星を抽出
    mask = angular_sep_deg <= radius_deg
    extracted = df[mask].copy()
    extracted['angular_sep_arcmin'] = angular_sep_deg[mask] * 60
    
    return extracted.sort_values('angular_sep_arcmin')

# Haversine版で抽出（より正確）
extracted_stars = extract_stars_haversine(
    your_dataframe, 
    target_ra, 
    target_dec, 
    search_radius_deg
)

print(f"抽出された星の数（Haversine）: {len(extracted_stars)}")
```

---

## Step 2: NEOWISEデータを取得してSQLiteに保存

### セル4: データベース作成関数

```python
def create_neowise_database(db_path):
    """NEOWISE用SQLiteデータベースを作成"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # sourcesテーブル
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS sources (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_id TEXT UNIQUE NOT NULL,
            ra REAL NOT NULL,
            dec REAL NOT NULL,
            allwise_cntr INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # neowise_raw_observationsテーブル（生データ）
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS neowise_raw_observations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_id TEXT NOT NULL,
            mjd REAL NOT NULL,
            band TEXT NOT NULL,
            mpro REAL,
            sigmpro REAL,
            cc_flags TEXT,
            ph_qual TEXT,
            moon_masked TEXT,
            sso_flg INTEGER,
            qi_fact REAL,
            saa_sep REAL,
            sat REAL,
            rchi2 REAL,
            qual_frame REAL,
            sky REAL,
            scan_id TEXT,
            mpro_corrected REAL,
            FOREIGN KEY (source_id) REFERENCES sources(source_id)
        )
    ''')
    
    # neowise_epoch_summaryテーブル（エポック集約データ）
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS neowise_epoch_summary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_id TEXT NOT NULL,
            band TEXT NOT NULL,
            epoch_id INTEGER NOT NULL,
            mjd_mean INTEGER NOT NULL,
            mag_mean REAL,
            mag_se REAL,
            mag_lim REAL,
            n_points INTEGER,
            snr REAL,
            filter_applied TEXT,
            FOREIGN KEY (source_id) REFERENCES sources(source_id)
        )
    ''')
    
    # インデックス
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_source_id ON sources(source_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_raw_source ON neowise_raw_observations(source_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_epoch_source ON neowise_epoch_summary(source_id)')
    
    conn.commit()
    return conn

print("データベース作成関数を定義しました")
```

### セル5: データ取得・保存関数

```python
# zp_stb（ゼロポイント補正テーブル）の読み込み
# パスはご自身の環境に合わせてください
try:
    zp_stb = pd.read_csv('/Users/yukikojima/Downloads/NEOWISE_zp_stb.csv', skiprows=12).rename(columns={'scan': 'scan_id'})
    print(f"zp_stb loaded: {len(zp_stb)} entries")
except FileNotFoundError:
    zp_stb = None
    print("zp_stb not found, skipping zero-point correction")


def save_raw_observations(raw_df, source_id, zp_stb_df, cursor):
    """生データをSQLiteに保存（等級は小数点以下4桁に丸め）"""
    for band in ['W1', 'W2']:
        band_lower = band.lower()
        mag_col = f'{band_lower}mpro'
        unc_col = f'{band_lower}sigmpro'
        sat_col = f'{band_lower}sat'
        rchi2_col = f'{band_lower}rchi2'
        sky_col = f'{band_lower}sky'
        dmag_col = f'{band_lower}dmag'
        
        band_df = raw_df.dropna(subset=[mag_col]).copy()
        if band_df.empty:
            continue
        
        # ゼロポイント補正
        if zp_stb_df is not None and dmag_col in zp_stb_df.columns:
            band_df = band_df.merge(zp_stb_df[['scan_id', dmag_col]], on='scan_id', how='left')
            band_df['mpro_corrected'] = band_df[mag_col] - band_df[dmag_col].fillna(0)
        else:
            band_df['mpro_corrected'] = band_df[mag_col]
        
        # SQLiteに挿入
        for _, row in band_df.iterrows():
            mpro_rounded = round(row[mag_col], 4) if pd.notna(row[mag_col]) else None
            sigmpro_rounded = round(row[unc_col], 4) if pd.notna(row[unc_col]) else None
            mpro_corrected_rounded = round(row['mpro_corrected'], 4) if pd.notna(row['mpro_corrected']) else None
            
            cursor.execute('''
                INSERT INTO neowise_raw_observations 
                (source_id, mjd, band, mpro, sigmpro, cc_flags, ph_qual, moon_masked,
                 sso_flg, qi_fact, saa_sep, sat, rchi2, qual_frame, sky, scan_id, mpro_corrected)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                source_id, row['mjd'], band,
                mpro_rounded, sigmpro_rounded,
                row.get('cc_flags', ''), row.get('ph_qual', ''), row.get('moon_masked', ''),
                int(row.get('sso_flg', 0)), row.get('qi_fact', 1.0), row.get('saa_sep', 0.0),
                row.get(sat_col, 0.0), row.get(rchi2_col, 0.0), row.get('qual_frame', 0.0),
                row.get(sky_col) if pd.notna(row.get(sky_col)) else None,
                row.get('scan_id', ''), mpro_corrected_rounded
            ))


def process_band_and_save_epochs(table_df, band, source_id, zp_stb_df, cursor):
    """デフォルトフィルタを適用してエポック集約データを保存"""
    band_lower = band.lower()
    mag_col = f'{band_lower}mpro'
    unc_col = f'{band_lower}sigmpro'
    sat_col = f'{band_lower}sat'
    rchi2_col = f'{band_lower}rchi2'
    sky_col = f'{band_lower}sky'
    dmag_col = f'{band_lower}dmag'
    cc_flag_idx = 0 if band == 'W1' else 1
    ph_qual_idx = 0 if band == 'W1' else 1
    moon_mask_idx = 0 if band == 'W1' else 1
    
    table_band = table_df.dropna(subset=[mag_col]).copy()
    if table_band.empty:
        return 0
    
    # デフォルトフィルタ適用
    try:
        table_filtered = table_band[
            (table_band['cc_flags'].str[cc_flag_idx] == '0') &
            (table_band['sso_flg'] == 0) &
            (table_band['qi_fact'] == 1.0) &
            (table_band['saa_sep'] >= 5.0) &
            (table_band['ph_qual'].str[ph_qual_idx] == 'A') &
            (table_band['moon_masked'].str[moon_mask_idx] == '0') &
            (table_band[sat_col] <= 0.05) &
            (table_band[rchi2_col] <= 50) &
            (table_band['qual_frame'] > 0.0) &
            (table_band[sky_col].notna())
        ].reset_index(drop=True).copy()
    except Exception:
        return 0
    
    if table_filtered.empty:
        return 0
    
    # ゼロポイント補正
    if zp_stb_df is not None and dmag_col in zp_stb_df.columns:
        table_filtered = table_filtered.merge(zp_stb_df[['scan_id', dmag_col]], on='scan_id', how='left')
        table_filtered[mag_col] -= table_filtered[dmag_col].fillna(0)
    
    # 3σクリッピング
    mean_mag, std_mag = table_filtered[mag_col].mean(), table_filtered[mag_col].std()
    if std_mag > 0:
        table_3sigma = table_filtered[
            (table_filtered[mag_col] >= (mean_mag - 3*std_mag)) &
            (table_filtered[mag_col] <= (mean_mag + 3*std_mag))
        ].copy()
    else:
        table_3sigma = table_filtered.copy()
    
    if table_3sigma.empty:
        return 0
    
    # フラックス計算
    table_3sigma['flux'] = 10**(-0.4 * table_3sigma[mag_col])
    table_3sigma['flux_error'] = table_3sigma['flux'] * (10**(0.4 * table_3sigma[unc_col]) - 1)
    
    # エポックID付与
    table_3sigma['epoch_id'] = (table_3sigma['mjd'].diff() >= 100).cumsum()
    
    # S/N計算
    epoch_stats = table_3sigma.groupby('epoch_id').agg(
        flux_sum=('flux', 'sum'),
        flux_error_sq_sum=('flux_error', lambda x: (x**2).sum())
    )
    with np.errstate(divide='ignore', invalid='ignore'):
        epoch_stats['snr'] = epoch_stats['flux_sum'] / np.sqrt(epoch_stats['flux_error_sq_sum'])
    
    good_epoch_ids = epoch_stats[epoch_stats['snr'] >= 100].index
    if good_epoch_ids.empty:
        good_epoch_ids = epoch_stats.index
    
    good_data = table_3sigma[table_3sigma['epoch_id'].isin(good_epoch_ids)].copy()
    
    # エポック集約
    def std_error(x):
        return np.std(x, ddof=1) / np.sqrt(len(x)) if len(x) > 1 else 0.0
    
    good_data = good_data.rename(columns={mag_col: 'magnitude', unc_col: 'magnitude_error'})
    
    result = good_data.groupby('epoch_id').agg(
        mjd=('mjd', 'mean'),
        mag_mean=('magnitude', 'mean'),
        mag_se=('magnitude', std_error),
        n_points=('mjd', 'size'),
        flux_mean=('flux', 'mean'),
        flux_error_sq_sum=('flux_error', lambda x: (x**2).sum()),
        flux_count=('flux', 'size')
    ).reset_index()
    
    # mag_lim計算
    with np.errstate(divide='ignore', invalid='ignore'):
        ratio = (result['flux_mean'] - np.sqrt(result['flux_error_sq_sum']) / result['flux_count']) / result['flux_mean']
        result['mag_lim'] = -2.5 * np.log10(ratio)
    
    result['snr'] = epoch_stats.loc[result['epoch_id'], 'snr'].values
    result = result.drop(columns=['flux_mean', 'flux_error_sq_sum', 'flux_count'])
    
    # SQLiteに保存（丸め処理）
    for _, row in result.iterrows():
        mjd_mean_rounded = int(round(row['mjd']))
        mag_mean_rounded = round(row['mag_mean'], 4) if pd.notna(row['mag_mean']) else None
        mag_se_rounded = round(row['mag_se'], 4) if pd.notna(row['mag_se']) else None
        mag_lim_rounded = round(row['mag_lim'], 4) if pd.notna(row['mag_lim']) and not np.isnan(row['mag_lim']) else None
        snr_rounded = round(row['snr'], 2) if pd.notna(row['snr']) and not np.isnan(row['snr']) else None
        
        cursor.execute('''
            INSERT INTO neowise_epoch_summary 
            (source_id, band, epoch_id, mjd_mean, mag_mean, mag_se, mag_lim, n_points, snr, filter_applied)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (source_id, band, int(row['epoch_id']), mjd_mean_rounded,
              mag_mean_rounded, mag_se_rounded, mag_lim_rounded,
              int(row['n_points']), snr_rounded, 'default'))
    
    return len(result)


def get_neowise_and_save(ra, dec, source_id, conn, zp_stb_df):
    """NEOWISEデータを取得してSQLiteに保存"""
    cursor = conn.cursor()
    
    # IRSAからデータ取得
    try:
        import astropy.coordinates as coord
        table = Irsa.query_region(
            coord.SkyCoord(ra, dec, unit=(u.deg, u.deg)), 
            catalog='neowiser_p1bs_psd', 
            radius='0d0m5s'
        )
        table.sort('mjd')
        raw_df = table.to_pandas()
    except Exception as e:
        print(f"  Error querying IRSA: {e}")
        return False
    
    if raw_df.empty:
        print(f"  No data found")
        return False
    
    if len(set(raw_df['allwise_cntr'])) != 1:
        print(f"  Multiple objects found")
        return False
    
    allwise_cntr = raw_df['allwise_cntr'].iloc[0]
    
    # sourcesテーブルに登録
    cursor.execute('''
        INSERT OR IGNORE INTO sources (source_id, ra, dec, allwise_cntr)
        VALUES (?, ?, ?, ?)
    ''', (source_id, ra, dec, int(allwise_cntr)))
    
    # mjdフィルタリング
    if zp_stb_df is not None and not zp_stb_df.empty:
        raw_df = raw_df[raw_df['mjd'] > zp_stb_df['mjd'].min()].reset_index(drop=True)
    
    if raw_df.empty:
        print(f"  No data after MJD filtering")
        return False
    
    # 生データを保存
    save_raw_observations(raw_df, source_id, zp_stb_df, cursor)
    
    # エポック集約データを計算・保存
    w1_epochs = process_band_and_save_epochs(raw_df.copy(), 'W1', source_id, zp_stb_df, cursor)
    w2_epochs = process_band_and_save_epochs(raw_df.copy(), 'W2', source_id, zp_stb_df, cursor)
    
    conn.commit()
    print(f"  Saved: W1={w1_epochs} epochs, W2={w2_epochs} epochs")
    return True

print("データ取得・保存関数を定義しました")
```

### セル6: NEOWISEデータ取得・保存を実行

```python
# 出力するSQLiteファイルのパス
db_path = "neowise_target_region.db"

# データベース作成
conn = create_neowise_database(db_path)

# 抽出した星のリストを準備
# extracted_starsからソースリストを作成
sources = [
    (str(row['source_id']), row['ra'], row['dec']) 
    for _, row in extracted_stars.iterrows()
]

print(f"処理する星の数: {len(sources)}")
print(f"出力ファイル: {db_path}")
print("-" * 50)

# 各星のNEOWISEデータを取得・保存
success_count = 0
error_count = 0

for source_id, ra, dec in tqdm(sources, desc="Processing"):
    print(f"\n{source_id}: RA={ra:.4f}, Dec={dec:.4f}")
    try:
        if get_neowise_and_save(ra, dec, source_id, conn, zp_stb):
            success_count += 1
        else:
            error_count += 1
    except Exception as e:
        print(f"  Error: {e}")
        error_count += 1

conn.close()

print("\n" + "=" * 50)
print(f"処理完了!")
print(f"  成功: {success_count}")
print(f"  エラー: {error_count}")
print(f"  出力: {db_path}")
```

---

## Step 3: 保存したデータを確認

### セル7: データベースの内容を確認

```python
# SQLiteファイルを開いて内容を確認
conn = sqlite3.connect(db_path)

# 天体数
sources_count = pd.read_sql_query("SELECT COUNT(*) as count FROM sources", conn)
print(f"登録された天体数: {sources_count['count'].iloc[0]}")

# 天体リスト
sources_df = pd.read_sql_query("SELECT * FROM sources", conn)
print(f"\n登録された天体:")
print(sources_df)

# 生データ件数
raw_count = pd.read_sql_query("""
    SELECT source_id, band, COUNT(*) as count 
    FROM neowise_raw_observations 
    GROUP BY source_id, band
""", conn)
print(f"\n生データ件数:")
print(raw_count)

# エポック集約データ
epoch_data = pd.read_sql_query("""
    SELECT source_id, band, mjd_mean, mag_mean, mag_se, n_points 
    FROM neowise_epoch_summary 
    ORDER BY source_id, band, mjd_mean
""", conn)
print(f"\nエポック集約データ:")
print(epoch_data)

conn.close()
```

---

## Step 4: ビューワーで表示

### 4.1 バックエンドを起動

ターミナルで以下を実行：

```bash
cd prototype/backend
pip install -r requirements.txt
python app.py
```

### 4.2 作成したSQLiteを使用するバックエンド

以下のコードを `prototype/backend/app_custom.py` として保存：

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import sqlite3
import pandas as pd

app = FastAPI(title="NEOWISE Lightcurve API")

# CORS設定
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# SQLiteファイルパス（作成したファイルのパスに変更）
DB_PATH = "neowise_target_region.db"

@app.get("/")
def root():
    return {"message": "NEOWISE Lightcurve API"}

@app.get("/api/list")
def list_sources():
    """登録された天体一覧を取得"""
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query("SELECT * FROM sources", conn)
    conn.close()
    return df.to_dict(orient='records')

@app.get("/api/lightcurve/neowise/{source_id}")
def get_neowise_lightcurve(source_id: str, raw: bool = False):
    """
    NEOWISEライトカーブを取得
    
    Parameters:
    - source_id: 天体識別子
    - raw: True=生データ, False=エポック集約データ（デフォルト）
    """
    conn = sqlite3.connect(DB_PATH)
    
    # 天体情報を取得
    source = pd.read_sql_query(
        "SELECT * FROM sources WHERE source_id = ?", 
        conn, params=[source_id]
    )
    
    if source.empty:
        conn.close()
        raise HTTPException(status_code=404, detail=f"Source not found: {source_id}")
    
    if raw:
        # 生データを取得
        data = pd.read_sql_query("""
            SELECT mjd, band, mpro_corrected as mag, sigmpro as mag_err,
                   cc_flags, ph_qual, sso_flg, sat, rchi2
            FROM neowise_raw_observations
            WHERE source_id = ?
            ORDER BY mjd
        """, conn, params=[source_id])
    else:
        # エポック集約データを取得
        data = pd.read_sql_query("""
            SELECT mjd_mean as mjd, band, mag_mean as mag, mag_se as mag_err, 
                   mag_lim, n_points, snr
            FROM neowise_epoch_summary
            WHERE source_id = ?
            ORDER BY mjd_mean
        """, conn, params=[source_id])
    
    conn.close()
    
    # W1とW2に分割
    w1_data = data[data['band'] == 'W1'].to_dict(orient='records')
    w2_data = data[data['band'] == 'W2'].to_dict(orient='records')
    
    return {
        "source_id": source_id,
        "ra": float(source['ra'].iloc[0]),
        "dec": float(source['dec'].iloc[0]),
        "w1": {
            "mjd": [d['mjd'] for d in w1_data],
            "mag": [d['mag'] for d in w1_data],
            "mag_err": [d['mag_err'] for d in w1_data]
        },
        "w2": {
            "mjd": [d['mjd'] for d in w2_data],
            "mag": [d['mag'] for d in w2_data],
            "mag_err": [d['mag_err'] for d in w2_data]
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 4.3 カスタムバックエンドを起動

```bash
# SQLiteファイルをbackendディレクトリにコピー
cp neowise_target_region.db prototype/backend/

# バックエンドを起動
cd prototype/backend
python app_custom.py
```

### 4.4 フロントエンドを起動

別のターミナルで：

```bash
cd prototype
python3 -m http.server 8080
```

### 4.5 ブラウザでアクセス

1. ブラウザで http://localhost:8080/ を開く
2. SOURCE IDフィールドに取得した天体のsource_idを入力
3. 「データを取得」ボタンをクリック
4. ライトカーブが表示される

---

## 補足: 並列処理で高速化

多くの星を処理する場合は、並列処理を使用すると大幅に高速化できます。

### 並列処理版（改良版）

以下のコードでは、以下の改良を行っています：
- **ログ出力**: 処理状況をロギング
- **コネクションプーリング**: requestsセッションのコネクションプールとリトライ設定
- **セマフォ**: 同時IRSAクエリ数を制限（レート制限対策）
- **リトライロジック**: エラー時に指数バックオフでリトライ

```python
import logging
import sqlite3
import threading
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# astro imports
import astropy.coordinates as coord
import astropy.units as u
from astroquery.ipac.irsa import Irsa

# ログ設定
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# DB ロック（sqlite はスレッドセーフでないので必要）
db_lock = threading.Lock()

# セマフォで「同時に発行する IRSA クエリ数」を制限（executor の workers とは別）
MAX_CONCURRENT_QUERIES = 4
query_semaphore = threading.BoundedSemaphore(MAX_CONCURRENT_QUERIES)


def prepare_irsa_session(pool_maxsize=50, max_retries=3, backoff_factor=1.0):
    """requests セッションを作って Irsa に流用（コネクションプールと retry 設定）"""
    session = requests.Session()
    retries = Retry(
        total=max_retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=frozenset(['GET', 'POST'])
    )
    adapter = HTTPAdapter(pool_connections=pool_maxsize, pool_maxsize=pool_maxsize, max_retries=retries)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    # astroquery uses Irsa._session internally; set it so all queries reuse this session
    Irsa._session = session
    logging.info("Prepared Irsa._session with pool_maxsize=%s", pool_maxsize)
    return session


def get_neowise_threadsafe(ra, dec, source_id, db_path, zp_stb_df, max_attempts=4):
    """並列処理用（セマフォとリトライ対応）"""
    logging.info(f"START source {source_id}")
    conn = sqlite3.connect(db_path, check_same_thread=False, timeout=10)
    cursor = conn.cursor()
    
    for attempt in range(max_attempts):
        try:
            # セマフォでIRSAクエリ数を制限
            with query_semaphore:
                table = Irsa.query_region(
                    coord.SkyCoord(ra, dec, unit=(u.deg, u.deg)), 
                    catalog='neowiser_p1bs_psd', 
                    radius='0d0m5s'
                )
                table.sort('mjd')
                raw_df = table.to_pandas()
            
            if raw_df.empty or len(set(raw_df['allwise_cntr'])) != 1:
                conn.close()
                return (source_id, False, "No valid data")
            
            allwise_cntr = raw_df['allwise_cntr'].iloc[0]
            
            with db_lock:
                cursor.execute('''
                    INSERT OR IGNORE INTO sources (source_id, ra, dec, allwise_cntr)
                    VALUES (?, ?, ?, ?)
                ''', (source_id, ra, dec, int(allwise_cntr)))
            
            if zp_stb_df is not None and not zp_stb_df.empty:
                raw_df = raw_df[raw_df['mjd'] > zp_stb_df['mjd'].min()].reset_index(drop=True)
            
            if raw_df.empty:
                conn.close()
                return (source_id, False, "No data after MJD filtering")
            
            with db_lock:
                save_raw_observations(raw_df, source_id, zp_stb_df, cursor)
                process_band_and_save_epochs(raw_df.copy(), 'W1', source_id, zp_stb_df, cursor)
                process_band_and_save_epochs(raw_df.copy(), 'W2', source_id, zp_stb_df, cursor)
                conn.commit()
            
            conn.close()
            logging.info(f"SUCCESS source {source_id}")
            return (source_id, True, "Success")
            
        except Exception as e:
            if attempt < max_attempts - 1:
                wait_time = 2 ** attempt  # exponential backoff
                logging.warning(f"Attempt {attempt+1} failed for {source_id}: {e}. Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                conn.close()
                logging.error(f"FAILED source {source_id} after {max_attempts} attempts: {e}")
                return (source_id, False, str(e))
    
    conn.close()
    return (source_id, False, "Max attempts exceeded")


# 並列処理実行
db_path = "neowise_target_region.db"
conn = create_neowise_database(db_path)
conn.close()

# IRSAセッションの準備
prepare_irsa_session(pool_maxsize=10, max_retries=3, backoff_factor=1.0)

num_workers = 4
print(f"Processing {len(sources)} sources with {num_workers} workers...")

with ThreadPoolExecutor(max_workers=num_workers) as executor:
    futures = {
        executor.submit(get_neowise_threadsafe, ra, dec, source_id, db_path, zp_stb): source_id
        for source_id, ra, dec in sources
    }
    
    for future in tqdm(as_completed(futures), total=len(futures)):
        source_id, success, msg = future.result()
        if not success:
            print(f"  {source_id}: {msg}")

print("完了!")
```

---

## データベースのクリア方法

再実行時に同じデータベースを使う場合、重複データが入らないようにデータベースをクリアできます。

### 方法1: Pythonで全データをクリア（テーブル構造は維持）

```python
def clear_database(db_path):
    """データベースの全データをクリア"""
    import sqlite3
    from pathlib import Path
    
    if not Path(db_path).exists():
        print(f"Database not found: {db_path}")
        return
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # 各テーブルのデータを削除
    tables = ['neowise_epoch_summary', 'neowise_raw_observations', 'sources']
    for table in tables:
        cursor.execute(f'DELETE FROM {table}')
        print(f"Cleared table: {table}")
    
    # VACUUMで空き容量を回収
    conn.execute('VACUUM')
    conn.commit()
    conn.close()
    print(f"Database cleared: {db_path}")

# 使用例
clear_database("neowise_target_region.db")
```

### 方法2: データベースファイルを削除して新規作成

```python
import os
from pathlib import Path

db_path = "neowise_target_region.db"

# ファイルを削除
if Path(db_path).exists():
    os.remove(db_path)
    print(f"Deleted: {db_path}")

# 新規作成
conn = create_neowise_database(db_path)
conn.close()
print(f"Created new database: {db_path}")
```

### 方法3: コマンドラインから

```bash
# データをクリア（テーブル構造は維持）
python neowise_to_sqlite.py --clear --output neowise_target_region.db

# データベースファイルを削除
python neowise_to_sqlite.py --drop --output neowise_target_region.db

# クリアしてから新しいデータを取得
python neowise_to_sqlite.py --clear --sources sources.csv --output neowise_target_region.db --parallel
```

---

## まとめ

このチュートリアルでは、以下の工程を説明しました：

1. **座標周辺の星を抽出**: DataFrameからHaversine公式を使って25分角以内の星を抽出
2. **NEOWISEデータ取得**: IRSAからデータを取得し、ゼロポイント補正を適用
3. **SQLiteに保存**: 生データとエポック集約データをSQLiteに保存
4. **ビューワー表示**: FastAPIバックエンドを起動してWebビューワーで表示
5. **データベース管理**: データのクリアと再実行方法

質問があればお知らせください！
